{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11bbf255",
   "metadata": {},
   "source": [
    "# AudioCLIP Demo\n",
    "\n",
    "Authored by [Andrey Guzhov](https://github.com/AndreyGuzhov)\n",
    "\n",
    "This notebook covers common use cases of AudioCLIP and provides the typical workflow.\n",
    "Below, you will find information on:\n",
    "\n",
    "0. [Binary Assets](#Downloading-Binary-Assets)\n",
    "1. [Required imports](#Imports-&-Constants)\n",
    "2. [Model Instantiation](#Model-Instantiation)\n",
    "3. [Data Transformation](#Audio-&-Image-Transforms)\n",
    "4. Data Loading\n",
    "    * [Audio](#Audio-Loading)\n",
    "    * [Images](#Image-Loading)\n",
    "5. [Preparation of the Input](#Input-Preparation)\n",
    "6. [Acquisition of the Output](#Obtaining-Embeddings)\n",
    "7. [Normalization of Embeddings](#Normalization-of-Embeddings)\n",
    "8. [Calculation of Logit Scales](#Obtaining-Logit-Scales)\n",
    "9. [Computation of Similarities](#Computing-Similarities)\n",
    "10. Performing Tasks\n",
    "    1. [Classification](#Classification)\n",
    "        1. [Audio](#Audio)\n",
    "        2. [Images](#Images)\n",
    "    2. [Querying](#Querying)\n",
    "        1. [Audio by Text](#Audio-by-Text)\n",
    "        2. [Images by Text](#Images-by-Text)\n",
    "        3. [Audio by Images](#Audio-by-Images)\n",
    "        4. [Images by Audio](#Images-by-Audio)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7262c7e",
   "metadata": {},
   "source": [
    "## Imports & Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "70dbdf30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import glob\n",
    "\n",
    "import librosa\n",
    "import librosa.display\n",
    "\n",
    "import simplejpeg\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torchvision as tv\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from PIL import Image\n",
    "from IPython.display import Audio, display\n",
    "\n",
    "sys.path.append(os.path.abspath(f'{os.getcwd()}/..')) # .ipynb 파일이 위치한 경로\n",
    "\n",
    "from model import AudioCLIP\n",
    "from utils.transforms import ToTensor1D\n",
    "\n",
    "\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "MODEL_FILENAME = 'AudioCLIP-Full-Training.pt'\n",
    "# derived from ESResNeXt\n",
    "SAMPLE_RATE = 44100\n",
    "# derived from CLIP\n",
    "IMAGE_SIZE = 224\n",
    "IMAGE_MEAN = 0.48145466, 0.4578275, 0.40821073\n",
    "IMAGE_STD = 0.26862954, 0.26130258, 0.27577711\n",
    "\n",
    "LABELS = ['cat', 'thunderstorm', 'coughing', 'alarm clock', 'car horn'] # audio 개수와 일치하지 않아도 되는건가?, 학습 시에는 맞춰줘야 함(train /val split 때문에)\n",
    "print(type(LABELS))\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a327cc6",
   "metadata": {},
   "source": [
    "## Model Instantiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "f398f22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "aclp = AudioCLIP(pretrained=f'../assets/{MODEL_FILENAME}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39421f88",
   "metadata": {},
   "source": [
    "## Audio & Image Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "dd4d76b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_transforms = ToTensor1D()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5beab9",
   "metadata": {},
   "source": [
    "## Audio Loading\n",
    "Audio samples are drawn from the [ESC-50](https://github.com/karolpiczak/ESC-50) dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "5aaa79b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['audio_segment/coughing_1-58792-A-24_segment_1.wav', 'audio_segment/cat_3-95694-A-5_segment_0.wav', 'audio_segment/cat_3-95694-A-5_segment_1.wav', 'audio_segment/car_horn_1-24074-A-43_segment_1.wav', 'audio_segment/coughing_1-58792-A-24_segment_0.wav', 'audio_segment/thunder_3-144891-B-19_segment_1.wav', 'audio_segment/car_horn_1-24074-A-43_segment_0.wav', 'audio_segment/alarm_clock_3-120526-B-37_segment_0.wav', 'audio_segment/alarm_clock_3-120526-B-37_segment_1.wav', 'audio_segment/thunder_3-144891-B-19_segment_0.wav']\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "paths_to_audio = glob.glob('audio_segment/*.wav') # audio 파일이 위치한 경로\n",
    "\n",
    "audio = list()\n",
    "for path_to_audio in paths_to_audio:\n",
    "    track, _ = librosa.load(path_to_audio, sr=SAMPLE_RATE, dtype=np.float32)\n",
    "\n",
    "    # compute spectrograms using trained audio-head (fbsp-layer of ESResNeXt)\n",
    "    # thus, the actual time-frequency representation will be visualized\n",
    "    spec = aclp.audio.spectrogram(torch.from_numpy(track.reshape(1, 1, -1)))\n",
    "    spec = np.ascontiguousarray(spec.numpy()).view(np.complex64)\n",
    "    pow_spec = 10 * np.log10(np.abs(spec) ** 2 + 1e-18).squeeze()\n",
    "\n",
    "    audio.append((track, pow_spec))\n",
    "\n",
    "print(paths_to_audio) # 어떤 순서로 loading 되는지는 잘 모르겠음\n",
    "print(len(paths_to_audio))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c11976",
   "metadata": {},
   "source": [
    "## Input Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "bbf1059b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AudioCLIP handles raw audio on input, so the input shape is [batch x channels x duration]\n",
    "audio = torch.stack([audio_transforms(track.reshape(1, -1)) for track, _ in audio])\n",
    "# text\n",
    "text = [[label] for label in LABELS]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afbb9ef5",
   "metadata": {},
   "source": [
    "## Obtaining Embeddings\n",
    "For the sake of clarity, all three modalities are processed separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "568f4eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for text encoding \n",
    "from model.model_final import FrozenCLIPTextEmbedder, Mapping_Model # text encoder\n",
    "text_encoder = FrozenCLIPTextEmbedder(version='RN50', device=device)\n",
    "\n",
    "#text_features = torch.stack([text_encoder.encode([label]).to(device).float() for label in LABELS])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "60c71e0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 1024])\n",
      "torch.Size([5, 1024])\n",
      "torch.Size([5, 1024])\n",
      "torch.Size([5, 1024])\n",
      "torch.Size([5, 1024])\n",
      "torch.Size([5, 1024])\n"
     ]
    }
   ],
   "source": [
    "# AudioCLIP's output: Tuple[Tuple[Features, Logits], Loss]\n",
    "# Features = Tuple[AudioFeatures, ImageFeatures, TextFeatures]\n",
    "# Logits = Tuple[AudioImageLogits, AudioTextLogits, ImageTextLogits]\n",
    "\n",
    "((audio_features, _, _), _), _ = aclp(audio=audio) # audio embedding \n",
    "print(audio_features.shape)\n",
    "((_, _, text_features), _), _ = aclp(text=text) # text embedding\n",
    "for i in text_features: \n",
    "    print(text_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e45ed0",
   "metadata": {},
   "source": [
    "## Normalization of Embeddings\n",
    "The AudioCLIP's output is normalized using L<sub>2</sub>-norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "f9758c7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nomalized audio embedding shape: torch.Size([10, 1024])\n",
      "tensor([[-0.0054,  0.0168, -0.0171,  ..., -0.0010, -0.0028,  0.0119],\n",
      "        [ 0.0124, -0.0051, -0.0022,  ..., -0.0160,  0.0204, -0.0063],\n",
      "        [-0.0062, -0.0080, -0.0636,  ..., -0.0152,  0.0046, -0.0099],\n",
      "        [-0.0233, -0.0438, -0.0098,  ..., -0.0057, -0.0173,  0.0046],\n",
      "        [ 0.0153, -0.0070, -0.0421,  ...,  0.0153, -0.0002, -0.0064]])\n"
     ]
    }
   ],
   "source": [
    "audio_features = audio_features / torch.linalg.norm(audio_features, dim=-1, keepdim=True)\n",
    "print(f'nomalized audio embedding shape: {audio_features.shape}') # normalization 한다고 해서 차원이 달라지진 않음\n",
    "text_features = text_features / torch.linalg.norm(text_features, dim=-1, keepdim=True)\n",
    "print(text_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c92adfb5",
   "metadata": {},
   "source": [
    "## Obtaining Logit Scales\n",
    "Outputs of the text-, image- and audio-heads are made consistent using dedicated scaling terms for each pair of modalities.\n",
    "The scaling factors are clamped between 1.0 and 100.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "12a89e64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(100.)\n"
     ]
    }
   ],
   "source": [
    "# audio와 text만 고려\n",
    "scale_audio_text = torch.clamp(aclp.logit_scale_at.exp(), min=1.0, max=100.0)\n",
    "# print(f'scaled embedding: {scale_audio_text}')\n",
    "print(scale_audio_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e3dfd0",
   "metadata": {},
   "source": [
    "## Computing Similarities\n",
    "Similarities between different representations of a same concept are computed using [scaled](#Obtaining-Logit-Scales) dot product (cosine similarity)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "c3121148",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 3.7929, -0.5249,  3.7583,  1.0434,  6.3707],\n",
      "        [ 7.0164,  0.4689, -0.3424, -0.4053,  3.7883],\n",
      "        [ 9.2904, -1.1262, -5.1571, -4.6368,  0.7858],\n",
      "        [ 1.6267,  1.0276,  2.8426,  0.4084,  5.9047],\n",
      "        [-0.6302,  0.2372,  6.4231,  1.0683,  0.6146],\n",
      "        [ 1.9733,  7.2482,  1.7210,  0.0743,  2.7171],\n",
      "        [ 2.4561,  2.8463,  2.7906,  1.4245,  5.3515],\n",
      "        [ 3.1987,  2.7979,  0.6837,  8.2739,  3.6359],\n",
      "        [ 2.7705,  3.0641,  0.2023,  8.5307,  3.8250],\n",
      "        [ 1.7586,  7.9345, -1.5157, -1.7790,  1.9068]])\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "# audio와 text 간의 similality 계산\n",
    "logits_audio_text = scale_audio_text * audio_features @ text_features.T\n",
    "print(logits_audio_text)\n",
    "print(logits_audio_text.dim())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a0dfa2",
   "metadata": {},
   "source": [
    "## Classification\n",
    "This task is a specific case of a more general one, which is [querying](#Querying).\n",
    "However, this setup is mentioned as a standalone because it demonstrates clearly how to perform usual classification (including [zero-shot inference](https://github.com/openai/CLIP#zero-shot-prediction)) using AudioCLIP."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bdd15af",
   "metadata": {},
   "source": [
    "### Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "6ccc74da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tFilename, Audio\t\t\tTextual Label (Confidence)\n",
      "\n",
      "coughing_1-58792-A-24_segment_1.wav ->\t\t       car horn (86.57%),             cat (06.57%),        coughing (06.35%)\n",
      " cat_3-95694-A-5_segment_0.wav ->\t\t            cat (95.94%),        car horn (03.80%),    thunderstorm (00.14%)\n",
      " cat_3-95694-A-5_segment_1.wav ->\t\t            cat (99.98%),        car horn (00.02%),    thunderstorm (00.00%)\n",
      "car_horn_1-24074-A-43_segment_1.wav ->\t\t       car horn (93.25%),        coughing (04.36%),             cat (01.29%)\n",
      "coughing_1-58792-A-24_segment_0.wav ->\t\t       coughing (98.95%),     alarm clock (00.47%),        car horn (00.30%)\n",
      "thunder_3-144891-B-19_segment_1.wav ->\t\t   thunderstorm (97.98%),        car horn (01.06%),             cat (00.50%)\n",
      "car_horn_1-24074-A-43_segment_0.wav ->\t\t       car horn (81.05%),    thunderstorm (06.62%),        coughing (06.26%)\n",
      "alarm_clock_3-120526-B-37_segment_0.wav ->\t\t    alarm clock (97.98%),        car horn (00.95%),             cat (00.61%)\n",
      "alarm_clock_3-120526-B-37_segment_1.wav ->\t\t    alarm clock (98.36%),        car horn (00.89%),    thunderstorm (00.42%)\n",
      "thunder_3-144891-B-19_segment_0.wav ->\t\t   thunderstorm (99.54%),        car horn (00.24%),             cat (00.21%)\n"
     ]
    }
   ],
   "source": [
    "print('\\t\\tFilename, Audio\\t\\t\\tTextual Label (Confidence)', end='\\n\\n')\n",
    "\n",
    "# calculate model confidence\n",
    "confidence = logits_audio_text.softmax(dim=1)\n",
    "# print(f'confidence score: {confidence}')\n",
    "\n",
    "for audio_idx in range(len(paths_to_audio)):\n",
    "    # acquire Top-3 most similar results\n",
    "    conf_values, ids = confidence[audio_idx].topk(3) # ids의 type은 tensor\n",
    "\n",
    "    # format output strings\n",
    "    query = f'{os.path.basename(paths_to_audio[audio_idx]):>30s} ->\\t\\t'\n",
    "    results = ', '.join([f'{LABELS[i]:>15s} ({v:06.2%})' for v, i in zip(conf_values, ids)])\n",
    "\n",
    "    # print(conf_values) # 결과마다 다 다른 tensor 값이 나와야 함. -> 제대로 된 결과\n",
    "\n",
    "    print(query + results)\n",
    "\n",
    "    # classification 결과로 봤을 때, data의 개수와 label의 개수가 일치하지 않아도 되는 것인가?\n",
    "    # 즉, 중복을 제거하고 각각 하나의 label만 존재해도 되는듯 싶음."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49a1a06",
   "metadata": {},
   "source": [
    "## Querying"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b939334c",
   "metadata": {},
   "source": [
    "### Audio by Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "d1e0b4ff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tTextual Label\t\tFilename, Audio (Confidence)\n",
      "\n",
      "                      cat ->\t\t cat_3-95694-A-5_segment_1.wav (89.80%),  cat_3-95694-A-5_segment_0.wav (09.24%)\n",
      "             thunderstorm ->\t\tthunder_3-144891-B-19_segment_0.wav (65.55%), thunder_3-144891-B-19_segment_1.wav (33.00%)\n",
      "                 coughing ->\t\tcoughing_1-58792-A-24_segment_0.wav (87.74%), coughing_1-58792-A-24_segment_1.wav (06.11%)\n",
      "              alarm clock ->\t\talarm_clock_3-120526-B-37_segment_1.wav (56.30%), alarm_clock_3-120526-B-37_segment_0.wav (43.55%)\n",
      "                 car horn ->\t\tcoughing_1-58792-A-24_segment_1.wav (44.41%), car_horn_1-24074-A-43_segment_1.wav (27.87%)\n"
     ]
    }
   ],
   "source": [
    "print('\\t\\tTextual Label\\t\\tFilename, Audio (Confidence)', end='\\n\\n')\n",
    "\n",
    "# calculate model confidence\n",
    "confidence = logits_audio_text.softmax(dim=0)\n",
    "for label_idx in range(len(LABELS)):\n",
    "    # acquire Top-2 most similar results\n",
    "    conf_values, ids = confidence[:, label_idx].topk(2)\n",
    "\n",
    "    # format output strings\n",
    "    query = f'{LABELS[label_idx]:>25s} ->\\t\\t'\n",
    "    results = ', '.join([f'{os.path.basename(paths_to_audio[i]):>30s} ({v:06.2%})' for v, i in zip(conf_values, ids)])\n",
    "\n",
    "    print(query + results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
